{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from NeuroPredictor.FeatExtractor import (\n",
    "    TimmFeatureExtractor,\n",
    "    TorchvisionFeatureExtractor,\n",
    "    CLIPFeatureExtractor,\n",
    "    OpenCLIPFeatureExtractor\n",
    ")\n",
    "from NeuroPredictor.Encoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "#      Hyperparameters\n",
    "# ==============================\n",
    "device        = 'cuda'\n",
    "batch_size    = 32\n",
    "\n",
    "# 训练集（MEI 与神经反应对应）\n",
    "train_img_dir = r\"D:\\Analysis\\NSD_Alignment\\NSD_shared1000\"\n",
    "train_resp    = r\"D:\\Analysis\\Ephys_data_Face.npz\"   # shape (N_train, n_neurons)\n",
    "# 测试集（MEI 生成后用于检验的图像）\n",
    "test_img_dir  = r\"D:\\Analysis\\results\\figures_face_timm\"\n",
    "save_dir = r\"D:\\Analysis\\results\"\n",
    "\n",
    "# 第一组 Encoder（用于生成 MEI 时用的那个）\n",
    "PRIMARY_ENCODER_CFG = {\n",
    "    'model_type': 'timm',             # 'timm', 'torchvision', 'clip', 'open_clip'\n",
    "    'model_name': 'vit_base_patch16_clip_224.laion2b',\n",
    "    'model_layer': 'blocks.8'\n",
    "}\n",
    "\n",
    "# 第二组 Encoder（用于比较的其它几个）\n",
    "SECONDARY_ENCODERS = [\n",
    "    {\n",
    "        'name': 'ResNet50_layer4',\n",
    "        'model_type': 'torchvision',\n",
    "        'model_name': 'resnet50',\n",
    "        'model_layer': 'layer4.2.relu'\n",
    "    },\n",
    "    {\n",
    "        'name': 'CLIP_RN50_layer4',\n",
    "        'model_type': 'clip',\n",
    "        'model_name': 'RN50',\n",
    "        'model_layer': 'layer4.2.relu3'\n",
    "    },\n",
    "    {\n",
    "        'name': 'OpenCLIP_ViT32_layer9',\n",
    "        'model_type': 'open_clip',\n",
    "        'model_name': 'ViT-B/32',\n",
    "        'model_layer': 'transformer.resblocks.9'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c460610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "#       Utility Classes\n",
    "# ==============================\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.paths = sorted([\n",
    "            os.path.join(folder, f)\n",
    "            for f in os.listdir(folder)\n",
    "            if f.lower().endswith(('.png','jpg','jpeg','bmp','tiff'))\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# ==============================\n",
    "#   Backbone Factory & Config\n",
    "# ==============================\n",
    "BACKBONE_CONFIG = {\n",
    "    'timm': {\n",
    "        'class': TimmFeatureExtractor,\n",
    "        'init_kwargs': {'model_name': None},  # fill in later\n",
    "        'hook_arg': 'layer_or_names',\n",
    "        'embedtype': 'spatial',\n",
    "        'transform': transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.48145466,0.4578275,0.40821073],\n",
    "                std =[0.26862954,0.26130258,0.27577711]\n",
    "            )\n",
    "        ])\n",
    "    },\n",
    "    'torchvision': {\n",
    "        'class': TorchvisionFeatureExtractor,\n",
    "        'init_kwargs': {'model_name': None},\n",
    "        'hook_arg': 'module_names',\n",
    "        'transform': None\n",
    "    },\n",
    "    'clip': {\n",
    "        'class': CLIPFeatureExtractor,\n",
    "        'init_kwargs': {'model_name': None},\n",
    "        'hook_arg': 'module_names',\n",
    "        'transform': None\n",
    "    },\n",
    "    'open_clip': {\n",
    "        'class': OpenCLIPFeatureExtractor,\n",
    "        'init_kwargs': {'model_name': None},\n",
    "        'hook_arg': 'module_names',\n",
    "        'transform': None\n",
    "    }\n",
    "}\n",
    "\n",
    "def make_extractor(cfg):\n",
    "    bc = BACKBONE_CONFIG[cfg['model_type']]\n",
    "    bc['init_kwargs']['model_name'] = cfg['model_name']\n",
    "    extractor = bc['class'](**bc['init_kwargs'])\n",
    "    extractor.to(device).eval()\n",
    "    for p in extractor.parameters(): p.requires_grad = False\n",
    "    hook_args = { bc['hook_arg']: cfg['model_layer'] }\n",
    "    return extractor, hook_args, bc['transform'] or extractor.get_preprocess()\n",
    "\n",
    "def extract_all_features(extractor, hook_args, transform, img_dir):\n",
    "    ds = ImageFolderDataset(img_dir, transform=transform)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            out  = extractor(imgs, **hook_args)\n",
    "            feats.append(out.cpu().numpy())\n",
    "    return np.concatenate(feats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(train_resp)['data']      # shape (N_train, n_neurons)\n",
    "\n",
    "# --- Build primary encoder and fit ---\n",
    "primary_cfg = PRIMARY_ENCODER_CFG\n",
    "ext_p, hook_p, trans_p = make_extractor(primary_cfg)\n",
    "X_train = extract_all_features(ext_p, hook_p, trans_p, train_img_dir)\n",
    "if len(X_train.shape) > 2:\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "enc_primary = Encoder(method='Ridge', cv_folds=5)\n",
    "enc_primary.fit(X_train, y_train)\n",
    "y_pred_primary = enc_primary.predict(X_train)\n",
    "\n",
    "# --- Prepare test features ---\n",
    "test_feats = {}\n",
    "for enc_cfg in [PRIMARY_ENCODER_CFG] + SECONDARY_ENCODERS:\n",
    "    ext, hook, trans = make_extractor(enc_cfg)\n",
    "    feats = extract_all_features(ext, hook, trans, test_img_dir)\n",
    "    if len(feats.shape) > 2:\n",
    "        feats = feats.reshape(feats.shape[0], -1)\n",
    "    test_feats[enc_cfg.get('name','primary')] = feats\n",
    "\n",
    "# --- Predict on test set ---\n",
    "preds = {}\n",
    "for name, feats in test_feats.items():\n",
    "    if name == 'primary':\n",
    "        preds[name] = enc_primary.predict(feats)\n",
    "    else:\n",
    "        enc = Encoder(method='Ridge', cv_folds=5)\n",
    "        # use same responses for training\n",
    "        ext_cfg = next(e for e in SECONDARY_ENCODERS if e['name']==name)\n",
    "        # fit on train\n",
    "        Xtr = extract_all_features(*make_extractor(ext_cfg), train_img_dir)\n",
    "        if len(Xtr.shape) > 2:\n",
    "            Xtr = Xtr.reshape(Xtr.shape[0], -1)\n",
    "        enc.fit(Xtr, y_train)\n",
    "        preds[name] = enc.predict(feats)\n",
    "    print(name, preds[name].shape)\n",
    "\n",
    "# --- Visualization ---\n",
    "# os.makedirs('results', exist_ok=True)\n",
    "for name, y_pred in preds.items():\n",
    "    if name == PRIMARY_ENCODER_CFG.get('name','primary'):\n",
    "        continue\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(\n",
    "        np.mean(preds['primary'], axis=1),\n",
    "        np.mean(y_pred, axis=1),\n",
    "        alpha=0.3, s=5\n",
    "    )\n",
    "    plt.plot([np.mean(y_pred, axis=1).min(), np.mean(y_pred, axis=1).max()],\n",
    "                [np.mean(y_pred, axis=1).min(), np.mean(y_pred, axis=1).max()],\n",
    "                'r--')\n",
    "    plt.xlabel('Primary Encoder Prediction')\n",
    "    plt.ylabel(f'{name} Prediction')\n",
    "    plt.title(f'Primary vs {name} on Test MEIs')\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'compare_primary_vs_{name}.png')\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
